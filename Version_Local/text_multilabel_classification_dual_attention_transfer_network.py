# -*- coding: utf-8 -*-
"""Text Multilabel Classification - Dual Attention Transfer Network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NKzOmVBwQoR366CnxfCvDv1p5ic2jPL1
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
from tqdm.notebook import tqdm
from pathlib import Path

import numbers
from datasets import load_dataset
import random
from sklearn import metrics, model_selection, preprocessing
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
import transformers
from transformers import AdamW, get_linear_schedule_with_warmup

from transformers import AutoTokenizer, BertModel
from transformers import XLNetTokenizer, XLNetModel

from dotmap import DotMap
from sklearn.metrics import f1_score, multilabel_confusion_matrix, confusion_matrix, ConfusionMatrixDisplay
from sklearn.model_selection import train_test_split
import seaborn as sns
import re
import gc


os.environ['CUDA_LAUNCH_BLOCKING'] = "1"

def seed(seed=42):
    random.seed(seed)
    os.environ['env'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    # some cudnn methods can be random even after fixing the seed unless you tell it to be deterministic
    torch.backends.cudnn.deterministic = True

seed()

# import wandb

# wandb.login()

# path = "/content/drive/MyDrive/CMU/S23/AI4SG/data/"
# path = "/content/drive/MyDrive/CMU/S23/AI4SG/Project/"
path = Path.cwd()
filename = "data_since_20221101_until_20230228_cleaned"
# data = pd.read_csv(os.path.join(path, "data", filename + ".csv"))
data = pd.read_csv("data_since_20221101_until_20230228_cleaned.csv")

data.info()
# exit()
data.subreddit.value_counts()

data.subreddit.unique()

# rename subreddit and grouping
selected_labels = [
                #    'SexualHarassment', 
                #    'domesticviolence',
                   'SuicideWatch', 
                   'ptsd', 
                   'abusiverelationships', 
                   'EatingDisorders',
                   'Others']
def regroup_label(label):
    if label in selected_labels:
        return label
    else:
        return "Others"

data.columns = ['title', 'selftext', 'labels', 'utc_datetime_str', 'created_utc', 'text']

data['labels'] = data['labels'].apply(lambda x: regroup_label(x))

# data = data[data['labels'] != 'depression']
data = data[data['labels'].isin(selected_labels)]

data.labels.unique()

train_df, test_df = train_test_split(data, test_size = 0.25, random_state = 4, shuffle = True, stratify = data['labels'])

train_df, valid_df = train_test_split(train_df, test_size = 0.25, random_state = 4, stratify = train_df['labels'])

print(train_df.shape, valid_df.shape, test_df.shape)

train_df.head()

train_df.reset_index(drop = True, inplace = True)
valid_df.reset_index(drop = True, inplace = True)
test_df.reset_index(drop = True, inplace = True)

mapping = {k:v for (k,v) in enumerate(selected_labels)}
mapping

"""------"""

sent = load_dataset("tweet_eval", "sentiment")

sent_train_df, sent_valid_df, sent_test_df = sent["train"].to_pandas(), sent["validation"].to_pandas(), sent["test"].to_pandas()

sent_mapping = {
    0: "negative",
    1: "neutral",
    2: "positive"
}

"""-----"""

n_labels = len(mapping)
sent_n_labels = len(sent_mapping)

# exit()

def class2idx(d):
    return {y: x for x, y in d.items()}

label_dict = class2idx(mapping)
sent_label_dict = class2idx(sent_mapping)

# def one_hot_encoder(df, n_labels):
#     one_hot_encoding = []
#     for i in tqdm(range(len(df))):
#         temp = [0]*n_labels
#         label_indices = df.iloc[i]["labels"]
#         for index in label_indices:
#             temp[index] = 1
#         one_hot_encoding.append(temp)
#     return pd.DataFrame(one_hot_encoding)

# def ohe(df, n_labels, dictionary, labels):
#     one_hot_encoding = []
#     for i in tqdm(range(len(df))):
#         temp = [0]*n_labels
#         label_index = df.iloc[i][labels]
#         try:
#             temp[dictionary[label_index]] = 1
#             one_hot_encoding.append(temp)
#         except:
#             continue
#     return pd.DataFrame(one_hot_encoding)

# def encode_index():
# exit()
def ohe_from_number(df, n_labels, dictionary, col_name):
    one_hot_encoding = []
    for i in tqdm(range(len(df))):
        temp = [0]*n_labels
        if not isinstance(df.iloc[i][col_name], numbers.Number):
            label_indices = df.iloc[i][col_name]
            for index in label_indices:
                temp[index] = 1
            one_hot_encoding.append(temp)
        else:
            temp[df.iloc[i][col_name]] = 1
            one_hot_encoding.append(temp)
    return pd.DataFrame(one_hot_encoding)

def ohe_from_string(df, n_labels, dictionary, col_name):
    one_hot_encoding = []
    for i in tqdm(range(len(df))):
        temp = [0]*n_labels
        if not isinstance(df.iloc[i][col_name], str):
            label_indices = df.iloc[i][col_name]
            for index in label_indices:
                temp[dictionary[index]] = 1
            one_hot_encoding.append(temp)
        else:
            temp[dictionary[df.iloc[i][col_name]]] = 1
            one_hot_encoding.append(temp)
    return pd.DataFrame(one_hot_encoding)


# # exit()
# train_ohe_labels = ohe_from_string(train_df, n_labels, label_dict, "labels")
# valid_ohe_labels = ohe_from_string(valid_df, n_labels, label_dict, "labels")
# test_ohe_labels = ohe_from_string(test_df, n_labels, label_dict, "labels")

# sent_train_ohe_labels = ohe_from_number(sent_train_df, sent_n_labels, sent_label_dict, "label")
# sent_valid_ohe_labels = ohe_from_number(sent_valid_df, sent_n_labels, sent_label_dict, "label")
# sent_test_ohe_labels = ohe_from_number(sent_test_df, sent_n_labels, sent_label_dict, "label")

# train_df = pd.concat([train_df, train_ohe_labels.iloc[:,:-1]], axis=1)
# valid_df = pd.concat([valid_df, valid_ohe_labels.iloc[:,:-1]], axis=1)
# test_df = pd.concat([test_df, test_ohe_labels.iloc[:,:-1]], axis=1)
# sent_train_df = pd.concat([sent_train_df, sent_train_ohe_labels], axis=1)
# sent_valid_df = pd.concat([sent_valid_df, sent_valid_ohe_labels], axis=1)
# sent_test_df = pd.concat([sent_test_df, sent_test_ohe_labels], axis=1)

# exit()

class TriggerDataset:
    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len
    
    def __len__(self):
        return len(self.texts)

    def __getitem__(self, index):
        text = self.texts[index]
        label = self.labels[index]

        inputs = self.tokenizer.__call__(text,
                                        None,
                                        add_special_tokens=True,
                                        max_length=self.max_len,
                                        padding="max_length",
                                        truncation=True,
                                        )
        ids = inputs["input_ids"]
        mask = inputs["attention_mask"]

        return {
            "ids": torch.tensor(ids, dtype=torch.long),
            "mask": torch.tensor(mask, dtype=torch.long),
            "labels": torch.tensor(label, dtype=torch.long)
        }

class SentimentDataset:
    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer 
        self.max_len = max_len 

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, index):
        text = self.texts[index]
        label = self.labels[index]
        inputs = self.tokenizer.__call__(self.texts[index],
                                       None, 
                                       add_special_tokens=True,
                                       max_length=self.max_len, 
                                       padding="max_length",
                                       truncation=True)
        ids = inputs["input_ids"]
        mask = inputs["attention_mask"]

        return {
            "ids": torch.tensor(ids, dtype=torch.long),
            "mask": torch.tensor(mask, dtype=torch.long),
            "labels": torch.tensor(label, dtype=torch.long)        
        }

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class SelfAttentionPooling(torch.nn.Module):
    ''' Scaled Dot-Product Attention based Pooling '''

    def __init__(self, temperature, attn_dropout=0.1):
        super().__init__()
        self.temperature = temperature
        self.dropout = torch.nn.Dropout(attn_dropout)

    def forward(self, q, k, v, mask=None):
        attn = torch.matmul(q / self.temperature, k.transpose(1, 2))

        if mask is not None:
            attn = attn.masked_fill(mask == 0, -1e9)

        unnormalized_attn = attn
        attn_weights = F.softmax(unnormalized_attn, dim=-1)
        attn = self.dropout(attn_weights)
        output = torch.matmul(attn, v)

        return torch.sum(output, dim=1), unnormalized_attn # output, attention weight

class TriggerClassifier(torch.nn.Module):
    def __init__(self, n_train_steps, n_classes, sent_n_classes, do_prob, dim, model, sent_model):
        super().__init__()

        self.model = model
        self.sent_model = sent_model 

        self.dropout = nn.Dropout(do_prob)
        self.pooler = SelfAttentionPooling(temperature=768**0.5)
        self.fc1 = nn.Linear(768*2, 768)
        self.fc2 = nn.Linear(768, 768)

        self.out = nn.Linear(768, n_classes)
        self.sent_out = nn.Linear(768, sent_n_classes)
        self.softmax = nn.Softmax()
        self.sigmoid = nn.Sigmoid()
        self.relu = nn.ReLU()

        self.out_dv = nn.Linear(768, 1)
        self.out_suicide = nn.Linear(768, 1)
        self.out_ptsd = nn.Linear(768, 1)
        self.out_abusive = nn.Linear(768, 1)
        self.out_eating = nn.Linear(768, 1)
        self.out_others = nn.Linear(768, 1)

        # self.n_train_steps = n_train_steps
        # self.step_scheduler_after = "batch"

    def forward(self, ids, mask, sent_ids, sent_mask):
        output = self.model(ids, attention_mask=mask)["last_hidden_state"] # (B, seq_len, 768)
        sent_output = self.model(sent_ids, attention_mask=sent_mask)["last_hidden_state"]

        output = self.dropout(output)
        sent_output = self.dropout(sent_output)
        output, attn_weights = self.pooler(output + sent_output, output + sent_output, output + sent_output) # output = (B, 1, emb_size)
        sent_output, sent_attn_weights = self.pooler(sent_output, sent_output, sent_output)

        fin_rep = output
        sent_fin_rep = sent_output 

        sent_output = self.dropout(sent_output)
        sent_output = self.sent_out(sent_output)
        sent_output = self.softmax(sent_output)

        # import pdb; pdb.set_trace()

        output = self.fc1(torch.cat((fin_rep, sent_fin_rep), dim=1))
        output = self.relu(output) 
        output = self.fc2(output)
        output = self.relu(output) 
        output = self.dropout(output)
        output = self.out(output)

        # output_dv = self.out_dv(output)
        # output_suicide = self.out_suicide(output)
        # output_ptsd = self.out_ptsd(output)
        # output_abusive = self.out_abusive(output)
        # output_eating = self.out_eating(output)
        # output_others = self.out_others(output)

        # dv_prediction = self.sigmoid(output_dv)
        # suicide_prediction = self.sigmoid(output_suicide)
        # ptsd_prediction = self.sigmoid(output_ptsd)
        # abusive_prediction = self.sigmoid(output_abusive)
        # eating_prediction = self.sigmoid(output_eating)
        # others_prediction = self.sigmoid(output_others)

        # predictions = [
        #     dv_prediction, 
        #     suicide_prediction, 
        #     ptsd_prediction, 
        #     abusive_prediction,
        #     eating_prediction, 
        #     others_prediction
        # ]

        # import pdb; pdb.set_trace()

        # output = torch.stack(predictions)

        # return output, attn_weights, fin_rep, sent_output, sent_attn_weights
        return output, attn_weights, fin_rep, sent_output, sent_attn_weights

def build_dataset(tokenizer_max_len, train, valid, test, n_labels):
    train_dataset = TriggerDataset(train.text.tolist(), train[range(n_labels)].values.tolist(), tokenizer, tokenizer_max_len)
    valid_dataset = TriggerDataset(valid.text.tolist(), valid[range(n_labels)].values.tolist(), tokenizer, tokenizer_max_len)
    test_dataset = TriggerDataset(test.text.tolist(), test[range(n_labels)].values.tolist(), tokenizer, tokenizer_max_len)
    return train_dataset, valid_dataset, test_dataset

def build_sent_dataset(tokenizer_max_len, train, valid, test, n_labels):
    train_dataset = SentimentDataset(train.text.tolist(), train[range(n_labels)].values.tolist(), tokenizer, tokenizer_max_len)
    valid_dataset = SentimentDataset(valid.text.tolist(), valid[range(n_labels)].values.tolist(), tokenizer, tokenizer_max_len)
    test_dataset = SentimentDataset(test.text.tolist(), test[range(n_labels)].values.tolist(), tokenizer, tokenizer_max_len)
    return train_dataset, valid_dataset, test_dataset

def build_dataloader(train_dataset, valid_dataset, test_dataset, batch_size):
    train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
    valid_data_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=1)
    test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=1)
    return train_data_loader, valid_data_loader, test_data_loader

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# define networks with the same underlying BERT model
bert = BertModel.from_pretrained("bert-base-uncased")

sample_train_dataset, _, _ = build_dataset(40, train_df, valid_df, test_df, n_labels-1)
print(sample_train_dataset[0])
len(sample_train_dataset)

# def ret_dual_optimizer(emotion_model, sentiment_model):
#     emo_param_optimizer = list(emotion_model.named_parameters())
#     # sent_param_optimizer = list(sentiment_model.named_parameters())
#     sent_param_optimizer = [p for p in sentiment_model.named_parameters() if p not in sentiment_model.model.named_parameters()]
#     print(len(emo_param_optimizer))
#     print(len(sent_param_optimizer))

#     no_decay = ["bias", "LayerNorm.bias"]
#     optimizer_parameters = [
#         {
#             "params": [
#                 p for n, p in emo_param_optimizer if not any(nd in n for nd in no_decay)
#             ],
#             "weight_decay": 0.001,
#         },
#         # {
#         #     "params": [
#         #         p for n, p in emo_param_optimizer if any(nd in n for nd in no_decay)
#         #     ],
#         #     "weight_decay": 0.0,
#         # },
#         # {
#         #     "params": [
#         #         p for n, p in sent_param_optimizer if not any(nd in n for nd in no_decay)
#         #     ],
#         #     "weight_decay": 0.001,
#         # },
#         {
#             "params": [
#                 p for n, p in sent_param_optimizer if any(nd in n for nd in no_decay)
#             ],
#             "weight_decay": 0.0,
#         },
#     ]
#     # opt = AdamW(optimizer_parameters, lr=wandb.config.learning_rate)
#     opt = AdamW(optimizer_parameters, lr=config.learning_rate)
#     return opt

# def ret_optimizer(model):
#     param_optimizer = list(model.named_parameters())

#     no_decay = ["bias", "LayerNorm.bias"]
#     optimizer_parameters = [
#         {
#             "params": [
#                 p for n, p in param_optimizer if not any(nd in n for nd in no_decay)
#             ],
#             "weight_decay": 0.001,
#         },
#         {
#             "params": [
#                 p for n, p in param_optimizer if any(nd in n for nd in no_decay)
#             ],
#             "weight_decay": 0.0,
#         },
#     ]
#     # opt = AdamW(optimizer_parameters, lr=wandb.config.learning_rate)
#     opt = AdamW(optimizer_parameters, lr=config.learning_rate)
#     return opt

def ret_scheduler(optimizer, num_train_steps):
    sch = get_linear_schedule_with_warmup(
        optimizer, num_warmup_steps=0, num_training_steps=num_train_steps)
    return sch

def loss_fn(outputs, labels):
    if labels is None:
        return None
    return nn.BCEWithLogitsLoss()(outputs, labels.float())

def log_metrics(preds, labels):
    preds = torch.stack(preds)
    preds = preds.cpu().detach().numpy()
    labels = torch.stack(labels)
    labels = labels.cpu().detach().numpy()

    fpr_micro, tpr_micro, _ = metrics.roc_curve(labels.ravel(), preds.ravel())
    
    auc_micro = metrics.auc(fpr_micro, tpr_micro)
    return {"auc_micro": auc_micro}

def f1_score_func(preds, labels, threshold):
    preds = torch.stack(preds)
    preds = preds.cpu().detach().numpy()
    labels = torch.stack(labels)
    labels = labels.cpu().detach().numpy()
    preds = np.where(preds > threshold, 1, 0)
    
    return f1_score(labels, preds, average='weighted')

def accuracy_per_class(preds, labels):
    preds = torch.stack(preds)
    preds = preds.cpu().detach().numpy()
    labels = torch.stack(labels)
    labels = labels.cpu().detach().numpy()

    for label in np.unique(labels):
        y_preds = preds[labels==label]
        y_true = labels[labels==label]
        print(f'Class: {mapping[label]}')
        # print(f'Class: {label_dict_inverse[label]}')
        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\n')

def train_fn(dataloader, sent_dataloader, model, optimizer, device, scheduler):
    train_total_loss = 0.0
    sent_train_loss = 0.0
    model.train()

    for idx, (data, sent_data) in tqdm(enumerate(zip(dataloader, sent_dataloader)), total=len(dataloader)):
        
        ids = data["ids"]
        mask = data["mask"]
        targets = data["labels"]

        ids = ids.to(device, dtype=torch.long)
        mask = mask.to(device, dtype=torch.long)
        targets = targets.to(device, dtype=torch.float)

        sent_ids = sent_data["ids"]
        sent_mask = sent_data["mask"]
        sent_targets = sent_data["labels"]

        sent_ids = sent_ids.to(device, dtype=torch.long)
        sent_mask = sent_mask.to(device, dtype=torch.long)
        sent_targets = sent_targets.to(device, dtype=torch.long)

        optimizer.zero_grad()
        
        # forward pass (emotion)
        outputs, attn_weights, _, sent_outputs, sent_attn_weights = model(ids=ids, mask=mask, sent_ids=sent_ids, sent_mask=sent_mask)

        loss = loss_fn(outputs, targets)
        sent_loss = loss_fn(sent_outputs, sent_targets)

        loss.backward(retain_graph=True)
        sent_loss.backward(retain_graph=True)

        total_loss = sent_loss.item() + loss.item()

        # train_loss += loss.item()
        # train_sent_loss += sent_loss.item()

        train_total_loss += total_loss

        optimizer.step()
        scheduler.step()

    del ids, mask, targets, sent_ids, sent_mask, sent_targets
    del outputs, attn_weights, sent_outputs, sent_attn_weights
    return train_total_loss

def eval_fn(dataloader, sent_dataloader, model, device):
    eval_total_loss = 0.0
    model.eval()
    fin_targets = []
    fin_outputs = []
    for idx, (data, sent_data) in tqdm(enumerate(zip(dataloader, sent_dataloader)), total=len(dataloader)):
        ids = data["ids"]
        mask = data["mask"]
        targets = data["labels"]

        ids = ids.to(device, dtype=torch.long)
        mask = mask.to(device, dtype=torch.long)
        targets = targets.to(device, dtype=torch.float)

        sent_ids = sent_data["ids"]
        sent_mask = sent_data["mask"]
        sent_targets = sent_data["labels"]

        sent_ids = sent_ids.to(device, dtype=torch.long)
        sent_mask = sent_mask.to(device, dtype=torch.long)
        sent_targets = sent_targets.to(device, dtype=torch.float)

        with torch.inference_mode():
            # forward pass
            outputs, attn_weights, _, sent_outputs, sent_attn_weights = model(ids=ids, mask=mask, sent_ids=sent_ids, sent_mask=sent_mask)

        loss = loss_fn(outputs, targets)
        sent_loss = loss_fn(sent_outputs, sent_targets)

        total_loss = loss.item() + sent_loss.item()

        eval_total_loss += total_loss

        fin_targets.extend(targets)
        fin_outputs.extend(torch.sigmoid(outputs))

    return eval_total_loss, fin_outputs, fin_targets

def pred_fn(dataloader, model, device):
    model.eval()
    fin_outputs = []
    fin_targets = []

    for idx, data in tqdm(enumerate(dataloader), total=len(dataloader)):
        ids = data["ids"]
        mask = data["mask"]
        targets = data["labels"]

        ids = ids.to(device, dtype=torch.long)
        mask = mask.to(device, dtype=torch.long)
        targets = targets.to(device, dtype=torch.float)

        with torch.inference_mode():
            outputs, _, _, _, _ = model(ids=ids, mask=mask, sent_ids=ids, sent_mask=mask)

        fin_targets.extend(targets)
        fin_outputs.extend(torch.sigmoid(outputs))
    return fin_outputs, fin_targets

config = {
    'learning_rate': 5e-6,
    'batch_size': 32,
    'epochs': 15,
    'dropout': 0.5,
    'tokenizer_max_len': 100
}
config = DotMap(config)

checkpoint_path = os.path.join(path, "model", "best_dual_model__v2.pt")

def trainer(train_dataloader, sent_train_dataloader, valid_dataloader, sent_valid_dataloader, model, optimizer, scheduler, config=None):
    # with wandb.init(config=config):
    #     config = wandb.config

        # wandb.watch(model)
        
    n_epochs = config.epochs

    best_val_loss = 100
    for epoch in tqdm(range(n_epochs)):
        train_loss = train_fn(train_dataloader, sent_train_dataloader, model, optimizer, device, scheduler)
        eval_loss, preds, labels = eval_fn(valid_dataloader, sent_valid_dataloader, model, device) # preds output as probs

        auc_score = log_metrics(preds, labels)["auc_micro"]
        print("AUC score: ", auc_score)

        avg_train_loss, avg_val_loss = train_loss / len(train_dataloader), eval_loss / len(valid_dataloader)
        print("Average Train loss: ", avg_train_loss)
        print("Average Valid loss: ", avg_val_loss)

        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), checkpoint_path)
            
            print("Model saved as current val_loss is: ", best_val_loss)

# wandb.agent(sweep_id, function=trainer, count=6)

# accuracy_per_class(preds, label)

sent_train_df

train_size, valid_size, test_size = min(len(train_df), len(sent_train_df)), min(len(valid_df), len(sent_valid_df)), min(len(test_df), len(sent_test_df))

train_dataset, valid_dataset, test_dataset = build_dataset(config.tokenizer_max_len, train_df[:train_size], valid_df[:valid_size], test_df[:test_size], n_labels-1)
sent_train_dataset, sent_valid_dataset, sent_test_dataset = build_sent_dataset(config.tokenizer_max_len, sent_train_df[:train_size], sent_valid_df[:valid_size], sent_test_df[:test_size], sent_n_labels)
train_dataloader, valid_dataloader, test_dataloader = build_dataloader(train_dataset, valid_dataset, test_dataset, config.batch_size)
sent_train_dataloader, sent_valid_dataloader, sent_test_dataloader = build_dataloader(sent_train_dataset, sent_valid_dataset, sent_test_dataset, config.batch_size)

n_train_steps = int(len(train_dataset) / config.batch_size * 10)
print(len(train_dataset))

exit()

print(len(train_dataset), len(sent_train_dataset))
print(len(valid_dataset), len(sent_valid_dataset))
print(len(test_dataset), len(sent_test_dataset))

gc.collect()
torch.cuda.empty_cache()

del model, optimizer, scheduler

# sentiment_model = SentimentClassifier(n_train_steps, sent_n_labels, config.dropout, model=bert_model)
model = TriggerClassifier(n_train_steps, n_labels-1, sent_n_labels, config.dropout, dim=768, model=bert, sent_model=bert)

optimizer = torch.optim.AdamW(model.parameters(), lr = config['learning_rate'])
scheduler = ret_scheduler(optimizer, n_train_steps)
model.to(device)
model = nn.DataParallel(model)

trainer(train_dataloader, sent_train_dataloader, valid_dataloader, sent_valid_dataloader, model, optimizer, scheduler, config)

# model = SentimentClassifier(n_train_steps, sent_n_labels, config.dropout, model=bert_model)
# checkpoint_file = '/content/best_model.pt'
# if os.path.exists(checkpoint_file):
#     checkpoint = torch.load(checkpoint_file)
#     model.load_state_dict(checkpoint['model'])
#     optimizer.load_state_dict(checkpoint['optimizer'])
# model.eval()

if os.path.exists(checkpoint_path):
    model.load_state_dict(torch.load(checkpoint_path))
model.eval()

fin_outputs, fin_targets = pred_fn(test_dataloader, model, device)

all_targets = torch.stack(fin_targets).cpu().detach().numpy()
all_preds = torch.stack(fin_outputs).cpu().detach().numpy()

fin_outputs[-1]

threshold = 0.5
all_preds = np.where(all_preds > threshold, 1, 0)

list(label_dict.keys())[:-1]

from sklearn.metrics import classification_report
import pandas as pd
report = classification_report(all_targets, all_preds, target_names=list(label_dict.keys())[:-1], digits=4)
print(report) # some of the classes were never predicted

mcm = multilabel_confusion_matrix(all_targets, all_preds)

def print_confusion_matrix(confusion_matrix, axes, class_label, class_names, fontsize=14):
    df_cm = pd.DataFrame(
        confusion_matrix, index=class_names, columns=class_names,
    )

    try:
        heatmap = sns.heatmap(df_cm, annot=True, fmt="d", cbar=False, ax=axes)
    except ValueError:
        raise ValueError("Confusion matrix values must be integers.")
    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)
    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)
    axes.set_ylabel('True label')
    axes.set_xlabel('Predicted label')
    axes.set_title(class_label)
    # fig = heatmap.get_figure()
    # fig.savefig(class_label+".png")

fig, ax = plt.subplots(2, 3, figsize=(9, 7))
    
for axes, cfs_matrix, label in zip(ax.flatten(), mcm, label_dict.keys()):
    print_confusion_matrix(cfs_matrix, axes, label, ["0", "1"])

fig.savefig("confusion_matrix.png")

fig.tight_layout()
plt.show()

confusion_matrix(all_targets.argmax(axis=1), all_preds.argmax(axis=1))

fig, ax = plt.subplots(figsize=(9, 9))
display = ConfusionMatrixDisplay(confusion_matrix(all_targets.argmax(axis=1), all_preds.argmax(axis=1)), display_labels=label_dict.keys())
display.plot(ax = ax)
plt.show()

def inference(text, model, device, tokenizer, max_len=100):
    model.eval()
    input = tokenizer(text, 
                      None, 
                      add_special_tokens=True,
                      max_length=max_len,
                      padding="max_length",
                      truncation=True)
    id = torch.tensor(input["inputs_ids"], dtype=torch.long)
    mask = torch.tensor(input["attention_mask"], dtype=torch.long)
    id = id.to(device)
    mask = mask.to(device)

    with torch.inference_mode():
        outputs, _, _, _, _ = model(ids=id, mask=mask, sent_ids=id, sent_mask=mask)

    return torch.sigmoid(outputs)

